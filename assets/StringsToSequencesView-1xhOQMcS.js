import{d as a,o as e,c as d,a as r,_ as s}from"./index-BtpqPFez.js";import"./flowbite-DnTiltT1.js";const o="/assets/recall_vs_map50-BebkaQ8m.jpg",n="/assets/other-image-original-ru928AR7.jpg",i="/assets/other-image-non-frozen-C_WU2_Cx.jpg",l="/assets/other-image-finetuned-C5pUI0SH.jpg",c="/assets/dhimitrios-non-frozen-Dgw-PzGG.jpg",v="/assets/dhimitrios-DtLOzqmc.jpg",g="/assets/output_normal_image-BpBD8Csf.jpg",h="/assets/other-image-2-finetuned-BLDcPP49.jpg",p="/assets/occlusion_untrained-CxC-GpzF.jpg",b="/assets/occlusion_trained-B7Ib1I1A.jpg",m="/assets/occlusion_trained_full-CLMRl-z5.jpg",f=a({__name:"StringsToSequencesView",setup(u){return(k,t)=>(e(),d("div",null,t[0]||(t[0]=[r('<p class="mb-4" data-v-8d0011e7> This system automates <strong data-v-8d0011e7>chord recognition</strong> from acoustic guitar videos by detecting and classifying chords based on video input. It leverages <strong data-v-8d0011e7>YOLO</strong><a href="https://arxiv.org/abs/1506.02640" class="link" target="_blank" rel="noopener" data-v-8d0011e7> [Redmon et al., 2016] </a> and <strong data-v-8d0011e7>Faster R-CNN</strong><a href="https://arxiv.org/abs/1506.01497" class="link" target="_blank" rel="noopener" data-v-8d0011e7> [Ren et al., 2016] </a> for <em data-v-8d0011e7>fretboard</em> detection, allowing the system to identify the position of the hand and fingers on the guitar neck. For chord classification, it utilizes <strong data-v-8d0011e7>Vision Transformers</strong><a href="https://arxiv.org/abs/2010.11929" class="link" target="_blank" rel="noopener" data-v-8d0011e7> [Dosovitskiy et al., 2020] </a> and <strong data-v-8d0011e7>DINOv2</strong><a href="https://arxiv.org/abs/2304.07193" class="link" target="_blank" rel="noopener" data-v-8d0011e7> [Oquab et al.] </a> , which process visual cues to distinguish between different chords. Additionally, <strong data-v-8d0011e7>hand pose estimation</strong> with <a href="https://github.com/google-ai-edge/mediapipe" class="link" target="_blank" rel="noopener" data-v-8d0011e7>MediaPipe</a> was explored as a potential method to perform chord recognition. Finally, we extend the work from <a href="https://ph01.tci-thaijo.org/index.php/ecticit/article/view/254624" class="link" target="_blank" rel="noopener" data-v-8d0011e7> [Kristian et al., 2024] </a> by exploring the potential of using state-of-the-art deep learning models and techniques with an additional proposal for an audio generation module. </p><p class="mb-4" data-v-8d0011e7> This system was created <a href="https://dhimitriosduka1.github.io/" target="_blank" class="link" rel="noopener" data-v-8d0011e7>Dhimitrios Duka</a>, <a href="https://github.com/Kanakanajm" target="_blank" class="link" rel="noopener" data-v-8d0011e7>Honglu Ma</a> and myself, as the final project for the <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/teaching/courses-1/ss-2024-high-level-computer-vision" class="link" target="_blank" rel="noopener" data-v-8d0011e7> High-Level Computer Vision </a> course lectured by <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele" class="link" target="_blank" rel="noopener" data-v-8d0011e7> Prof. Dr. Bernt Schiele </a> at <a href="https://www.uni-saarland.de/" class="link" target="_blank" rel="noopener" data-v-8d0011e7> Saarland University </a> during the Sommer Semester 2024. </p><section class="section" data-v-8d0011e7><h2 class="subsection" data-v-8d0011e7>Fretboard Detection</h2><p class="mb-4" data-v-8d0011e7> The table below shows the performance metrics of the different models tested on the finetuning dataset (<a href="https://universe.roboflow.com/hubert-drapeau-qt6ae/guitar-necks-detector/dataset/1" class="link" target="_blank" rel="noopener" data-v-8d0011e7>Guitar necks detector</a>), and the figure below shows the Recall vs. mAP@50 for the models tested and finetuned on the <em data-v-8d0011e7>fretboard</em> class, while showcasing the number of parameters. Naturally, the models finetuned with a Frozen Backbone (FB) performed slightly worse than the models finetuned without a Frozen Backbone; this was expected since the latter had the advantage of being able to learn the new task from scratch, using all layers, while the former only trained a smaller classifier head. Since we wanted to retain the ability to recognize the other 80 valuable classes from the <a href="https://cocodataset.org/#home" class="link" target="_blank" rel="noopener" data-v-8d0011e7>COCO dataset</a>, we chose a model from the (FB) list, the YOLOv9 (FB) model, as the best model for our task. This model obtained the highest precision and, after re-evaluating on the COCO dataset + <em data-v-8d0011e7>fretboard class</em>, it delivered better results in terms of confusion matrix and Precision-Recall curve. </p><table class="w-full border-collapse mb-4 border-t-4 border-b-4 border-black dark:border-white" data-v-8d0011e7><caption class="mb-2 text-left" data-v-8d0011e7><strong data-v-8d0011e7>Table 1:</strong> Performance metrics of different models on the evaluation dataset, shown in percentages. Each column represents a specific metric: Precision, Recall, mAP50-95, and mAP50. (FB) denotes models fine-tuned with a Frozen Backbone. </caption><thead data-v-8d0011e7><tr class="border-t-2 border-b-2 p-1 border-black dark:border-white" data-v-8d0011e7><th data-v-8d0011e7><strong data-v-8d0011e7>Model</strong></th><th data-v-8d0011e7><strong data-v-8d0011e7>P</strong></th><th data-v-8d0011e7><strong data-v-8d0011e7>R</strong></th><th data-v-8d0011e7><strong data-v-8d0011e7>mAP50-95</strong></th><th data-v-8d0011e7><strong data-v-8d0011e7>mAP50</strong></th></tr></thead><tbody data-v-8d0011e7><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7><a href="https://github.com/autogyro/yolo-V8" target="_blank" rel="noopener" class="link" data-v-8d0011e7>YOLOv8 (m)</a></td><td data-v-8d0011e7><strong data-v-8d0011e7>98.9%</strong></td><td data-v-8d0011e7>93.0%</td><td data-v-8d0011e7><strong data-v-8d0011e7>88.7%</strong></td><td data-v-8d0011e7><strong data-v-8d0011e7>98.2%</strong></td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7><a href="https://github.com/WongKinYiu/yolov9" target="_blank" rel="noopener" class="link" data-v-8d0011e7>YOLOv9 (c)</a></td><td data-v-8d0011e7>96.4%</td><td data-v-8d0011e7><strong data-v-8d0011e7>96.8%</strong></td><td data-v-8d0011e7>85.3%</td><td data-v-8d0011e7>97.8%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7><a href="https://github.com/THU-MIG/yolov10" target="_blank" rel="noopener" class="link" data-v-8d0011e7>YOLOv10 (l)</a></td><td data-v-8d0011e7>94.2%</td><td data-v-8d0011e7>87.0%</td><td data-v-8d0011e7>80.0%</td><td data-v-8d0011e7>94.4%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7><a href="https://pytorch.org/vision/master/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#fasterrcnn-resnet50-fpn" target="_blank" rel="noopener" class="link" data-v-8d0011e7>Faster-RCNN-Resnet50</a></td><td data-v-8d0011e7>80.8%</td><td data-v-8d0011e7>82.4%</td><td data-v-8d0011e7>77.5%</td><td data-v-8d0011e7>94.0%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7><a href="https://pytorch.org/vision/master/models/generated/torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn.html#fasterrcnn-mobilenet-v3-large-fpn" target="_blank" rel="noopener" data-v-8d0011e7>Faster-RCNN-MobileNetv3</a></td><td data-v-8d0011e7>79.4%</td><td data-v-8d0011e7>81.6%</td><td data-v-8d0011e7>75.7%</td><td data-v-8d0011e7>94.9%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7>YOLOv8 (FB)</td><td data-v-8d0011e7>76.7%</td><td data-v-8d0011e7><strong data-v-8d0011e7>85.1%</strong></td><td data-v-8d0011e7>53.4%</td><td data-v-8d0011e7>87.8%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7>YOLOv9 (FB)</td><td data-v-8d0011e7><strong data-v-8d0011e7>82.4%</strong></td><td data-v-8d0011e7>74.7%</td><td data-v-8d0011e7>54.7%</td><td data-v-8d0011e7>87.0%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7>YOLOv10 (FB)</td><td data-v-8d0011e7>81.4%</td><td data-v-8d0011e7>84.0%</td><td data-v-8d0011e7><strong data-v-8d0011e7>71.2%</strong></td><td data-v-8d0011e7>89.9%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7>Faster-RCNN-Resnet50 (FB)</td><td data-v-8d0011e7>62.9%</td><td data-v-8d0011e7>66.3%</td><td data-v-8d0011e7>59.0%</td><td data-v-8d0011e7><strong data-v-8d0011e7>93.4%</strong></td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7>Faster-RCNN-MobileNetv3 (FB)</td><td data-v-8d0011e7>71.7%</td><td data-v-8d0011e7>73.6%</td><td data-v-8d0011e7>68.3%</td><td data-v-8d0011e7>93.0%</td></tr></tbody></table><figure data-v-8d0011e7><img src="'+o+'" alt="Recall vs mAP@50" data-v-8d0011e7><figcaption class="mt-2" data-v-8d0011e7><strong data-v-8d0011e7>Figure 1:</strong> Recall vs. mAP@50 for the models tested and finetuned on the <em data-v-8d0011e7>fretboard</em> class. </figcaption></figure><section class="section" data-v-8d0011e7><div data-v-8d0011e7><h2 class="subsubsection" data-v-8d0011e7>Qualitative Results</h2><p class="mb-4" data-v-8d0011e7> Some qualitative results are shown below, comparing the original YOLOv9 (c) prediction with the finetuned model and the model with a frozen backbone + classifier layer. </p><div class="space-y-4" data-v-8d0011e7><figure data-v-8d0011e7><div class="grid grid-cols-1 md:grid-cols-3 gap-4" data-v-8d0011e7><img src="'+n+'" alt="Original YOLOv9 prediction" class="w-full h-auto object-contain" data-v-8d0011e7><img src="'+i+'" alt="Full-finetuning" class="w-full h-auto object-contain" data-v-8d0011e7><img src="'+l+'" alt="Frozen backbone + Classifier layer" class="w-full h-auto object-contain" data-v-8d0011e7></div><figcaption class="mt-2 text-justify text-base" data-v-8d0011e7><strong data-v-8d0011e7>Figure 2</strong>: Comparison of YOLOv9 (c) predictions with different training approaches. From <strong data-v-8d0011e7>left</strong> to <strong data-v-8d0011e7>right</strong>: original prediction, with full-finetuning, and with a frozen backbone + classifier layer. </figcaption></figure><figure data-v-8d0011e7><div class="grid grid-cols-1 md:grid-cols-2 gap-4" data-v-8d0011e7><img src="'+c+'" alt="With full-finetuning" class="w-full h-auto object-contain" data-v-8d0011e7><img src="'+v+'" alt="Frozen backbone + Classifier layer" class="w-full h-auto object-contain" data-v-8d0011e7></div><figcaption class="mt-2 text-justify text-base" data-v-8d0011e7><strong data-v-8d0011e7>Figure 3</strong>: Comparison of full-finetuning vs. frozen backbone with classifier layer. <strong data-v-8d0011e7>Left</strong>: with full-finetuning, <strong data-v-8d0011e7>right</strong>: with frozen backbone + classifier layer. </figcaption></figure><figure data-v-8d0011e7><div class="grid grid-cols-1 md:grid-cols-2 gap-4 align-middle items-center" data-v-8d0011e7><img src="'+g+'" alt="Prediction on an image from the COCO dataset" class="w-full h-auto object-contain" data-v-8d0011e7><img src="'+h+'" alt="From the Penn-Fudan dataset" class="w-full h-auto object-contain" data-v-8d0011e7></div><figcaption class="mt-2 text-justify text-base" data-v-8d0011e7><strong data-v-8d0011e7>Figure 4</strong>: Model predictions on different datasets. <strong data-v-8d0011e7>Left</strong>: prediction on an image from the <a href="https://cocodataset.org/#home" class="link" target="_blank" rel="noopener" data-v-8d0011e7>COCO dataset</a>, <strong data-v-8d0011e7>right</strong>: from the <a href="https://www.cis.upenn.edu/~jshi/ped_html/" class="link" target="_blank" rel="noopener" data-v-8d0011e7>Penn-Fudan dataset</a>. </figcaption></figure></div></div></section></section><section class="section" data-v-8d0011e7><h2 class="subsection" data-v-8d0011e7>Guitar Chord Classification</h2><h2 class="subsubsection" data-v-8d0011e7>Hand Pose Estimation + Classifier</h2><p class="mb-4" data-v-8d0011e7> To evaluate our approach against those in our reference paper by <a href="https://ph01.tci-thaijo.org/index.php/ecticit/article/view/254624" class="link" target="_blank" rel="noopener" data-v-8d0011e7> [Kristian et al., 2024] </a>, we implemented the <a href="https://arxiv.org/abs/1602.07261" class="link" target="_blank" rel="noopener" data-v-8d0011e7> InceptionResNetv2</a> model as described by the authors. After training the model using the hyperparameters provided by <a href="https://ph01.tci-thaijo.org/index.php/ecticit/article/view/254624" class="link" target="_blank" rel="noopener" data-v-8d0011e7> [Kristian et al., 2024] </a> on our dataset, we obtained the results shown in the table below, which provided us with a baseline to compare our models against. Surprisingly, this approach performed well, achieving good accuracy during validation and testing on two datasets. However, the model struggled to generalize to the third dataset, which was created by us. This outcome was anticipated, as the samples in our dataset were out of the training distribution, and the model lacked the complexity needed to generalize to such data. </p><table class="w-full border-collapse mb-4 border-t-4 border-b-4 border-black dark:border-white" data-v-8d0011e7><caption class="mb-2 text-left" data-v-8d0011e7><strong data-v-8d0011e7>Table 2:</strong> Accuracy of the Hand Pose Estimation + Classifier in the test set of different datasets. Datasets used: <code data-v-8d0011e7>GC: Guitar_Chords</code>, <code data-v-8d0011e7>GCT: Guitar_Chords_Tiny</code>, <code data-v-8d0011e7>GCO: Guitar_Chords_Ours</code>. </caption><thead data-v-8d0011e7><tr class="border-t-2 border-b-2 p-1 border-black dark:border-white" data-v-8d0011e7><th data-v-8d0011e7><strong data-v-8d0011e7>Model</strong></th><th data-v-8d0011e7><strong data-v-8d0011e7>GC</strong></th><th data-v-8d0011e7><strong data-v-8d0011e7>GCT</strong></th><th data-v-8d0011e7><strong data-v-8d0011e7>GCO</strong></th></tr></thead><tbody data-v-8d0011e7><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7><a href="https://arxiv.org/abs/1602.07261" class="link" target="_blank" rel="noopener" data-v-8d0011e7> InceptionResNetv2</a></td><td data-v-8d0011e7>83.56%</td><td data-v-8d0011e7>68.63%</td><td data-v-8d0011e7>15.57%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7><code data-v-8d0011e7>SVM (C = 300)</code></td><td data-v-8d0011e7><strong data-v-8d0011e7>95.27%</strong></td><td data-v-8d0011e7><strong data-v-8d0011e7>85.71%</strong></td><td data-v-8d0011e7><strong data-v-8d0011e7>18.61%</strong></td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7><code data-v-8d0011e7>Random Forest (n_estimators = 200)</code></td><td data-v-8d0011e7>93.35%</td><td data-v-8d0011e7>52.41%</td><td data-v-8d0011e7>16.16%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7><code data-v-8d0011e7>MLP (hidden_layer_sizes = (100, 256, 100))</code></td><td data-v-8d0011e7>89.44%</td><td data-v-8d0011e7>78.57%</td><td data-v-8d0011e7>14.39%</td></tr></tbody></table><h2 class="subsubsection" data-v-8d0011e7>Classifier only approach</h2><p class="mb-4" data-v-8d0011e7> To address this limitation of the previous approach, we decided to explore more complex models, such as <a href="https://arxiv.org/abs/2010.11929" class="link" target="_blank" rel="noopener" data-v-8d0011e7> Vision Transformers </a> and <a href="https://arxiv.org/abs/2304.07193" class="link" target="_blank" rel="noopener" data-v-8d0011e7> DINOv2</a>, which is also available on <a href="https://huggingface.co/docs/transformers/model_doc/dinov2" class="link" target="_blank" rel="noopener" data-v-8d0011e7> Hugging Face</a>. The results of our experiments are summarized below: </p><table class="w-full border-collapse mb-4 border-t-4 border-b-4 border-black dark:border-white" data-v-8d0011e7><caption class="mb-2 text-left" data-v-8d0011e7><strong data-v-8d0011e7>Table 3:</strong> Accuracy of the Classifier-only approach on the test set of different datasets. </caption><thead data-v-8d0011e7><tr class="border-t-2 border-b-2 p-1 border-black dark:border-white" data-v-8d0011e7><th data-v-8d0011e7><strong data-v-8d0011e7>Model</strong></th><th data-v-8d0011e7><strong data-v-8d0011e7>GC</strong></th><th data-v-8d0011e7><strong data-v-8d0011e7>GCT</strong></th><th data-v-8d0011e7><strong data-v-8d0011e7>GCO</strong></th></tr></thead><tbody data-v-8d0011e7><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7><a href="https://arxiv.org/abs/1602.07261" class="link" target="_blank" rel="noopener" data-v-8d0011e7> InceptionResNetv2</a></td><td data-v-8d0011e7>83.56%</td><td data-v-8d0011e7>68.63%</td><td data-v-8d0011e7>15.57%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7>ViT-B/16</td><td data-v-8d0011e7><strong data-v-8d0011e7>98.96%</strong></td><td data-v-8d0011e7>85.29%</td><td data-v-8d0011e7>96.24%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7>ViT-B/32</td><td data-v-8d0011e7>93.07%</td><td data-v-8d0011e7>81.37%</td><td data-v-8d0011e7>95.83%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7>ViT-L/16</td><td data-v-8d0011e7>95.84%</td><td data-v-8d0011e7>81.37%</td><td data-v-8d0011e7>12.29%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7>ViT-L/32</td><td data-v-8d0011e7>77.03%</td><td data-v-8d0011e7>43.14%</td><td data-v-8d0011e7>13.43%</td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7>DINOv2-S</td><td data-v-8d0011e7>96.24%</td><td data-v-8d0011e7>88.24%</td><td data-v-8d0011e7><strong data-v-8d0011e7>98.18%</strong></td></tr><tr class="hover:bg-gray-100 dark:hover:bg-gray-800" data-v-8d0011e7><td data-v-8d0011e7>DINOv2-L</td><td data-v-8d0011e7>96.44%</td><td data-v-8d0011e7><strong data-v-8d0011e7>91.18%</strong></td><td data-v-8d0011e7>97.92%</td></tr></tbody></table><p class="mb-4" data-v-8d0011e7> ViT models show varying performance across different datasets. The base models perform exceptionally well, with high accuracy on all datasets. However, the larger models do not exhibit the same performance. We argue that this is happening because the available data is not sufficient to train the large version of the models effectively. Additionally, we can also observe that the patch 16 versions of the ViT models perform better than the patch 32 versions. This is likely due to the fact that the patch 16 versions have a higher resolution, which is important for accurately distinguishing between different hand positions. </p><p class="mb-4" data-v-8d0011e7> Moreover, both DINOv2 variants demonstrated strong and consistent performance across all datasets. The <code data-v-8d0011e7>DINOv2-L</code> model, in particular, achieved the highest accuracy on the <code data-v-8d0011e7>Guitar_Chords_Ours</code> dataset, slightly outperforming the small variant. The superior performance of DINOv2 can be attributed to its self-supervised learning approach. Unlike models pre-trained on <a href="https://www.image-net.org/" class="link" target="_blank" rel="noopener" data-v-8d0011e7>ImageNet</a>, which does not contain a specific class for <em data-v-8d0011e7>hands</em>, DINOv2&#39;s self-supervised learning enables it to learn more generic and transferable representations, leading to better generalization in our task. This enhanced generalization is further supported by attention visualizations of the model when applied to images from <code data-v-8d0011e7>Guitar_Chords_Ours</code> dataset, where the model correctly focuses on the hand performing the fretting, as evidenced by the following figures. </p><figure class="flex flex-col items-center align-middle mb-4" data-v-8d0011e7><img src="'+p+'" alt="Occlusion in untrained model" width="75%" data-v-8d0011e7><img src="'+b+'" alt="Occlusion in trained model" width="75%" data-v-8d0011e7><figcaption class="text-justify mt-2" data-v-8d0011e7><strong data-v-8d0011e7>Figure 5:</strong> Occlusion-based attribution <a href="https://arxiv.org/abs/2009.07896" class="link" target="_blank" rel="noopener" data-v-8d0011e7>[Kokhlikyan et al., 2020]</a> for model interpretability on a <code data-v-8d0011e7>74×389</code> input image using a stride of <code data-v-8d0011e7>8</code> and a sliding window of <code data-v-8d0011e7>30×30</code>, using <a href="https://captum.ai/tutorials/TorchVision_Interpret#3--Occlusion-based-attribution" class="link" target="_blank" rel="noopener" data-v-8d0011e7>Captum</a>. <strong data-v-8d0011e7>Top:</strong> Untrained DINOv2 model. <strong data-v-8d0011e7>Bottom:</strong> Our DINOv2 model. </figcaption></figure><figure class="flex flex-col mb-4 align-middle items-center" data-v-8d0011e7><img src="'+m+'" alt="Occlusion in trained model - full picture" width="75%" data-v-8d0011e7><figcaption class="text-justify mt-2" data-v-8d0011e7><strong data-v-8d0011e7>Figure 6:</strong> Our DINOv2 model on a <code data-v-8d0011e7>360×640</code> input image using a stride of <code data-v-8d0011e7>20</code> and a sliding window of <code data-v-8d0011e7>60×60</code>. </figcaption></figure><p data-v-8d0011e7> Overall, our proposed models outperformed the <a href="https://arxiv.org/abs/1602.07261" class="link" target="_blank" rel="noopener" data-v-8d0011e7> InceptionResNetv2</a> model, achieving higher accuracy across all datasets. This demonstrates the potential of using more advanced models for <span class="text-highlighted" data-v-8d0011e7>chord classification tasks</span>. </p></section>',4)])))}}),_=s(f,[["__scopeId","data-v-8d0011e7"]]);export{_ as default};
